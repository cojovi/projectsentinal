---
title: "How Compliance Became Theater: The New AI Safety Playbook Is Mostly Optics"
date: 2026-02-25 19:04:09
tags:
  - AI safety
  - compliance theater
  - voluntary frameworks
  - AI governance
  - regulatory oversight
categories:
  - Artificial Intelligence
feature: true
cover: https://github.com/cojovi/projectsentinal/blob/main/test/how-compliance-became-theater-the-new-ai-safety-pl.png?raw=true
image: https://github.com/cojovi/projectsentinal/blob/main/test/ProtocolSentinelbanner.png?raw=true
og_image: https://github.com/cojovi/projectsentinal/blob/main/test/ProtocolSentinelbanner.png?raw=true
---

<!-- alt: A stage spotlight shining on a stack of AI policy binders and audit scorecards, suggesting performance over substance -->

AI safety has entered its *performative arts* era. Everyone’s wearing a lanyard, carrying a “framework,” and speaking in solemn tones about “responsible innovation”—while the actual incentives to ship first and apologize later remain untouched (like a sacred corporate relic).

**TL;DR**: AI “safety” in 2025–2026 is often a legitimacy shield—scorecards, red-team writeups, and voluntary frameworks that look serious but rarely constrain deployment unless regulators force consequences.

<!-- more -->

## The New Safety Script: “We Take This Seriously” (Translation: Please Don’t Regulate Us)
The modern AI safety playbook is a familiar genre: a confident press release, a tasteful PDF, a framework acronym that sounds like it was generated by an overworked committee, and a promise that *trust us*, everything is under control.

Here’s the plain-English version of what’s happening:

- Companies and governments are **using safety language as legitimacy**—a way to signal responsibility without necessarily changing behavior.
- “Audits,” “evaluations,” and “red-teaming” are increasingly common, but often **voluntary, non-standardized, and non-disclosed**.
- Policy announcements can be big and loud yet still leave the core deployment incentives intact: speed, market share, and “number go up.”

**Translation:** Safety has become a brand feature.

This isn’t a claim that all AI governance is fake. Some teams do serious work. Some regulators are trying. But the overall trend—especially in fast-moving frontier model development—is that “compliance” often functions like stage lighting: it makes the performance look polished while hiding the messy cables.

A useful framing from the broader governance debate (echoed in pieces like *“Beyond Compliance Theater”* from Trustable) is that **process proofs are not safety proofs**. You can document that you ran an evaluation. That doesn’t prove the system is safe in the world, against adaptive adversaries, under real incentives.

> **Key Insight**: “We did an audit” is not the same as “we can demonstrate safety under defined conditions.” The first is a process claim; the second is an evidence claim.

---

## Scorecards, Evals, and the Spreadsheet Cosplay of Accountability
In 2025–2026, model evaluation scorecards are everywhere. They’re tidy, they’re legible, and they’re dangerously easy to mistake for enforcement.

**Fact:** The industry has expanded its use of **model evaluations**—benchmarks for things like harmful content generation, jailbreak susceptibility, bias, and sometimes cybersecurity-related capabilities. There are also **risk management frameworks** gaining adoption across sectors, frequently referencing concepts like governance, documentation, monitoring, and incident response (as summarized in general governance explainers like U.S. Legal Support’s overview and risk-readiness discussions like Kovrr’s).

**The boring detail that matters:** Many of these scorecards are **not binding**, **not standardized**, and **not audited by an independent authority with teeth**.

So what do we get?

- A model “passes” internal thresholds—thresholds the company chose.
- A scorecard is published—often without enough detail to reproduce the results.
- The model ships anyway—because the business plan does not include “wait until the evals feel spiritually complete.”

**Translation:** A scorecard can be a dashboard… or a decorative placemat.

### What gets lost in the scorecard glow
- **Benchmark gaming:** If a benchmark becomes important, it becomes optimizable. That’s not corruption; it’s math meeting incentives.
- **Context collapse:** A model can look “safe” in controlled prompts and still behave badly in real workflows.
- **Unknown unknowns:** Evaluations mostly measure what we already thought to test.

This is why critics argue for more “proof-like” approaches—clear claims, measurable guarantees, and adversarial testing that actually tries to break the system, not just check the box (a theme emphasized in Trustable’s critique of compliance theater).

> **Pro Tip**: If an AI safety report doesn’t specify **who tested**, **what they tested**, **what failed**, and **what changed**, it’s probably PR wearing a lab coat.

---

## Red-Teaming Without Disclosure: Security Theater’s Smarter Cousin
Red-teaming—structured adversarial testing—is one of the better tools we have. It’s also increasingly used as a rhetorical shield.

**Fact:** Many AI developers now run red-team exercises and publish high-level summaries. Some describe categories of risks tested (self-harm, hate content, illicit instructions, jailbreaks) and mention mitigations.

**Claim (often implied):** “We red-teamed it, therefore it’s safe.”

**Reality check:** Red-teaming is only as useful as:

- the **competence and independence** of the red team,
- the **scope** of what they’re allowed to test,
- the **time** they’re given,
- and whether results lead to **meaningful changes** rather than “noted for future work.”

And here’s the awkward part: red-team results are frequently **not fully disclosed**. Sometimes that’s for legitimate reasons (e.g., publishing exploit details can create harm). But non-disclosure also conveniently prevents outsiders from assessing whether the testing was rigorous or symbolic.

**Translation:** “We tested it” can mean anything from “we hired experts for weeks” to “we asked interns to try jailbreak prompts for an afternoon.”

### The Morgan Lewis angle: safety tools create legal risk too
A February 2026 Morgan Lewis analysis on using AI to improve safety highlights a less glamorous truth: deploying AI for safety/compliance can introduce **legal risks**—privacy, employment law issues, discrimination concerns, documentation and defensibility problems.

That matters because it creates a perverse incentive:

- Companies want to say they’re using AI for safety.
- They also want to avoid creating discoverable records showing what they knew and when.

**Opinion:** This can nudge organizations toward **high-level statements and minimal specifics**—the exact recipe for compliance theater.

> **Expert View**: Legal risk management often pushes firms toward careful documentation *and* careful non-documentation. Yes, that’s contradictory. Welcome to modern compliance.

---

## Voluntary Frameworks: The Soft Power Buffet (Eat What You Like, Skip the Vegetables)
Frameworks are not useless. They can align vocabulary, define controls, and give organizations a starting point. The problem is how they’re used in practice: as a substitute for enforceable obligations.

From broad governance discussions (like U.S. Legal Support’s overview) to risk readiness framing (like Kovrr’s compliance-resilience perspective), the common pitch is:

- establish **governance structures**,
- define **risk categories**,
- implement **controls**,
- monitor and improve.

All reasonable.

**Here’s the part people skip:** Voluntary frameworks often lack:

- **penalties** for non-compliance,
- **verification** by independent parties,
- **comparability** across firms,
- and **public transparency**.

So they become… vibes. Very professional vibes, but still vibes.

### Corporate compliance theater is evolving, not disappearing
Corporate Compliance Insights has argued that AI-powered checks might change compliance theater—potentially automating monitoring, reducing manual box-checking, and making compliance more continuous.

**Fact:** Automation can improve detection and consistency.

**But:** automation can also scale the theater.

- If the underlying policy is weak, AI helps you enforce weak policy faster.
- If the incentives reward “clean dashboards,” AI helps you generate clean dashboards.

**Translation:** You can automate accountability—or automate the appearance of it. The tool doesn’t pick; leadership does.

---

## Governments Get in on the Act: National Frameworks and the Control Grab
Now for the part where the state shows up and says, “Don’t worry, we’re here to help,” which historically is either true, false, or true in a way that comes with paperwork.

A major U.S. development: a White House presidential action dated **December 2025** titled *“Eliminating State Law Obstruction of National Artificial Intelligence Policy”* (per the WhiteHouse.gov posting). The thrust, as presented, is to reduce friction from a patchwork of state rules and move toward a more unified national approach.

**Fact:** The document frames state-level divergence as a potential obstacle to a national AI policy.

**What it means (without guessing beyond the text):** This reflects a real tension:

- **Pro:** Uniform national policy can reduce compliance chaos and prevent contradictory requirements.
- **Con:** Centralization can also consolidate control and narrow experimentation, especially if federal policy is shaped by the most powerful stakeholders.

**Translation:** “Consistency” can mean “clarity,” or it can mean “one set of rules written by the people with the best lobbyists.” Sometimes it’s both.

### The legitimacy loop
Governments face their own incentives:

- Announce a framework to show action.
- Create advisory bodies.
- Encourage “responsible innovation.”

But if enforcement is limited, under-resourced, or politically constrained, the result is a familiar pattern: **policy announcements that don’t change deployment incentives**.

This is like installing a “Please drive safely” sign on a racetrack. Nice sentiment. The cars are still going 180 mph.

---

## What This Means: The Practical Implications (for People Who Don’t Get Paid in Frameworks)
Let’s translate the theater into real-world consequences.

### For consumers and the public
- You may see more “trust” messaging without a clear way to verify claims.
- Safety failures will still happen, and the post-incident response may focus on “we followed our process.”

### For businesses buying AI
- Vendor risk becomes harder: everyone claims compliance, few provide **comparable evidence**.
- Legal exposure grows if you rely on vague assurances instead of documented controls.

### For regulators and policymakers
- Voluntary regimes can become a **stall tactic**.
- Without disclosure standards and auditability, enforcement agencies are stuck evaluating vibes and PDFs.

### For the AI labs themselves
- The competitive pressure to deploy remains the gravitational force.
- If safety is framed as “we did the checklist,” it will be treated like a checklist.

> **Key Insight**: The core problem is incentive alignment. If shipping faster is rewarded and safety failures are survivable, the system will optimize for shipping faster.

---

## So What Would Non-Theater Look Like? (Yes, It’s Hard. That’s the Point.)
Nobody gets to wave a wand and make AI risk vanish. But we can distinguish *optics* from *evidence*.

Here are concrete markers that move safety from performance to proof:

- **Clear, testable safety claims**: Not “safe and responsible,” but “resists X class of jailbreaks under Y conditions with Z measured confidence.”
- **Independent evaluations** with meaningful access: Third parties who can actually probe, not just read a summary.
- **Disclosure norms**: Publish methodologies, failure rates, and mitigations—redacted where necessary, but not replaced with marketing fog.
- **Enforcement hooks**: Procurement requirements, liability standards, regulator audit authority, or penalties for misrepresentation.
- **Post-deployment monitoring**: Because the world is where models go to embarrass everyone.

**Opinion:** The future of AI governance will be decided less by how many frameworks exist and more by whether someone, somewhere, has the authority to say: *No, you can’t deploy that yet—and yes, we can prove why.*

**Translation:** If there’s no consequence, it’s not governance. It’s journaling.

---

## Key Takeaways
- **Compliance theater is thriving**: In 2025–2026, AI safety language often functions as a legitimacy shield more than a constraint.
- **Scorecards aren’t enforcement**: Model evals can inform risk decisions, but without standards, disclosure, and consequences, they’re easily weaponized as optics.
- **Red-teaming helps—until it’s a black box**: Non-disclosed red-team results may be necessary for security, but they also prevent accountability.
- **Voluntary frameworks can be useful—and also a stalling tactic**: They standardize language, not necessarily outcomes.
- **Policy centralization is a double-edged sword**: National frameworks can reduce patchwork chaos while also consolidating control and narrowing oversight.
- **The fix is incentives + evidence**: Measurable claims, independent testing, disclosure norms, and enforceable consequences are what separate safety from stagecraft.

> **Pro Tip**: When you hear “We’re aligned with leading frameworks,” ask the only question that matters: **“What happens if you’re not?”**